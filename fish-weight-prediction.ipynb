{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n \n![header.png](https://raw.githubusercontent.com/satishgunjal/images/master/fish.png)\n\nIn this study I am using Python 3 environment to create a machine learning model to predict the weight of the fish based on the body measurement data of seven types of fish species. You can download the dataset from Kaggle. [Fish market](https://www.kaggle.com/aungpyaeap/fish-market)\n \nI am going to use Linear model from sklearn library. Since there are multiple features its **Multiple/Multi Variable Linear Regression** problem.\n \nI have documented the code and tried to explain every important concept or library I have using during this study. I hope it will be helpful.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Step 1: Import The Required Libraries\n* numpy : Numpy is the core library for scientific computing in Python. It is used for working with arrays and matrices.\n* pandas: Used for data manipulation and analysis\n* matplotlib : Itâ€™s plotting library, and we are going to use it for data visualization\n* seaborn : It is also data visualization library, based on matplotlib\n* linear_model: Sklearn linear regression model\n* train_test_split : helper function from Sklearn library for splitting the dataset","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Load The Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/fish-market/Fish.csv\")\nprint('Shape of dataset= ', df.shape) # To get no of rows and columns\ndf.head(5) # head(n) returns first n records only. Can also use sample(n) for random n records.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" \n# Step 3: Understand The Data\n* There are total 159 rows(training samples) and 7 columns in dataset. \n* Each column details are as below \n \n| Column Name | Details\n| ------------|--------------\n| Species     | Species name of fish \n| Weight      | Weight of fish in gram     \n| Length1     | Vertical length in CM\n| Length2     | Diagonal length in CM\n| Length3     | Cross length in CM\n| Height      | Height in CM\n| Width       | Diagonal width in CM   \n \n* Features/input values/independent variables are 'Species', 'Length1','Length2', 'Length3', 'Height' and 'Width'\n* Target/output value/dependent variable is 'Weight'\n* So, we have to estimate the weight of the fish based on its measurement values.\n \nLet's change the name of columns lenght1,length2 and length3  as per the content of it.","metadata":{}},{"cell_type":"code","source":"df.rename(columns={'Length1':'VerticalLen','Length2':'DiagonalLen','Length3':'CrossLen'},inplace = True) # 'inplace= true' to make change in current dataframe\ndf.sample(5) # Display random 5 records","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's print the detailed information about our dataset","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Data Analysis, Cleaning and Visualization\n\n## Check for missing values","metadata":{}},{"cell_type":"code","source":"# isna() will return 'True' is value is 'None' or 'numpy.NaN'\n# Characters such as empty strings '' or 'numpy.inf' are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True)\n# you can also use df.isnull()\ndf.isna().sum() # Get sum of all Nan values from each column\n#df.isna().values.any()  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good, there no null values in our dataset.","metadata":{}},{"cell_type":"markdown","source":"## Get count for each species","metadata":{}},{"cell_type":"code","source":"df.Species.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above function gives us required values but lets create dataframe for species so that we can use it for better visualization","metadata":{}},{"cell_type":"code","source":"df_sp = df.Species.value_counts()\ndf_sp = pd.DataFrame(df_sp)\ndf_sp.T \n# Note: Just like matrices. 'dataframe.T' will Transpose index and columns\n# I am using it just for saving vertical space and making notbook more readable","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x= df_sp.index, y = df_sp.Species) # df_sp.index will returns row labels of dataframe\nplt.xlabel('Species')\nplt.ylabel('Count of Species')\nplt.rcParams[\"figure.figsize\"] = (10,6)\nplt.title('Fish Count Based On Species')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Important Points\n \n* As you can see our dataset is very small. We have only 6 training example for 'Whitefish' species. \n* Ideal approach would be to divide the dataset and do the prediction for each species. But since we don't have enough data we will ignore the different species during our analysis.","metadata":{}},{"cell_type":"markdown","source":"## Using Domain Knowledge For Data Cleaning\n* Depending on the maximum and minimum weight of fish for each species we can very easily remove the outliers. But because of limited data we are going to ignore the individual species and treat them as one. \n* Now lets use some common sense and find and remove the training data where weight of fish is 0 or negative","metadata":{}},{"cell_type":"code","source":"df[df.Weight <= 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets drop the training data at row 40. Note: Anytime we make changes in dataframe we are going to increament the dataframe name by 1","metadata":{}},{"cell_type":"code","source":"df1 = df.drop([40])\nprint('New dimension of dataset is= ', df1.shape)\ndf1.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation Check\n* Correlation helps us investigate and establish relationships between variables\n* Note that high amount of correlation between independent variables suggest that linear regression estimation will be unreliable","metadata":{}},{"cell_type":"code","source":"df1.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (10,6) # Custom figure size in inches\nsns.heatmap(df1.corr(), annot =True)\nplt.title('Correlation Matrix')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading Correlation Matrix \n* Correlation coefficient range from -1 to +1\n* Sign(+/-) indicate the direction and amount indicate the strength of correlation\n* +1.00 means perfect positive relationship\n* 0.00 means no relationship\n* -1.00 means perfect negative relationship\n* The correlation between 'VerticalLen', 'DiagonalLen' and 'Crosslen' is almost 1. This will cause 'Multicolinearity' and if we don't take care of it, it may lead to unreliable predictions.\n \nLet's drop the 'VerticalLen', 'DiagonalLen' and 'Crosslen' column.","metadata":{}},{"cell_type":"code","source":"df2 = df1.drop(['VerticalLen', 'DiagonalLen', 'CrossLen'], axis =1) # Can also use axis = 'columns'\nprint('New dimension of dataset is= ', df2.shape)\ndf2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visulization Using Pairplot","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df2, kind = 'scatter', hue = 'Species')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above pair plot, we can see that there seems to be some correlations between  Height, Width and the Weight. Note that since we have multiple species the correlation between Height and Width of all species is not exactly linear with Weight.\n \nNow, since we have the final dataset ready lets analyze and remove the outliers if any","metadata":{}},{"cell_type":"markdown","source":"## Outlier Detection and Removal\n \n* Outlier is an extremely high or extremely low value in our data\n* We use below formula to identify the outlier\n  ```\n    ( Greater than Q3 + 1.5 * IQR ) OR ( Lower than Q1 -1.5 * IQR )\n \n    where,\n    Q1  = First quartile\n    Q3  = Third quartile\n    IQR = Interquartile range (Q3 - Q1)\n  ```\n \n* Lets use box plot for outlier visualization. \n* Vertical line on the left side of box plot represent the 'min' value of dataset and vertical line on right side of box plot represent the 'max' value of dataset. Any value which is outside this range is outlier and represented by '*'","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x=df2['Weight'])\nplt.title('Outlier Detection based on Weight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above plot its clear that there are three outlier as per the 'Weight' data. Lets create a function to find the index of these outliers.","metadata":{}},{"cell_type":"code","source":"def outlier_detection(dataframe):\n  Q1 = dataframe.quantile(0.25)\n  Q3 = dataframe.quantile(0.75)\n  IQR = Q3 - Q1\n  upper_end = Q3 + 1.5 * IQR\n  lower_end = Q1 - 1.5 * IQR \n  outlier = dataframe[(dataframe > upper_end) | (dataframe < lower_end)]\n  return outlier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outlier_detection(df2['Weight'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So based on 'Weight' data, index 142, 143 and 144 are outliers\n\nLets check for 'Height' data","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x =df2['Height'])\nplt.title('Outlier Detection based on Height')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no outlier so no need to call 'outlier_detection()' function.\n\nLets check for 'Width' data","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x = df2['Width'])\nplt.title('Outlier Detection based on Width')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no outlier so no need to call 'outlier_detection()' function.","metadata":{}},{"cell_type":"code","source":"df3 = df2.drop([142,143,144])\ndf3.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5: Build Machine Learning Model","metadata":{}},{"cell_type":"markdown","source":"## Create Feature Matrix X and Target Variable y","metadata":{}},{"cell_type":"code","source":"#X = df3.iloc[:,[2,3]] # Select columns using column index\nX = df3[['Height','Width']] # Select columns using column name\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y = df3.iloc[:,[1]] # Select columns using column index\ny = df3[['Weight']]\ny.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create test and train dataset\n* We will split the dataset, so that we can use one set of data for training the model and one set of data for testing the model\n* We will keep 20% of data for testing and 80% of data for training the model","metadata":{}},{"cell_type":"code","source":"X_train,X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state = 42) \n# Use paramter 'random_state=1' if you want keep results same everytime you execute above code\nprint('X_train dimension= ', X_train.shape)\nprint('X_test dimension= ', X_test.shape)\nprint('y_train dimension= ', y_train.shape)\nprint('y_train dimension= ', y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ordinary Least Squares Algorithm\n\n* Lets the train the model using Ordinary Least Squares Algorithm\n* This is one of the most basic linear regression algorithm.\n* Mathematical formula used by ordinary least square algorithm is as below,\n\n   ![ordinary_least_squares_formlua.png](https://github.com/satishgunjal/images/blob/master/ordinary_least_squares_formlua_1.png?raw=true)\n* The objective of Ordinary Least Square Algorithm is to minimize the residual sum of squares. Here the term residual means 'deviation of predicted value(Xw) from actual value(y)'\n* Note that, problem with ordinary least square model is size of coefficients increase exponentially with increase in model complexity","metadata":{}},{"cell_type":"code","source":"model = linear_model.LinearRegression()\nmodel.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Understanding Training Results\n* If training is successful then we get the result like above. Where all the default values used by LinearRgression() model are displayed. If required we can also pass these values in fit method. We are not going to change any of these values for now.\n* As per our hypothesis function, 'model' object contains the coef(slope of line) and intercept values","metadata":{}},{"cell_type":"code","source":"print('coef= ', model.coef_) # Since we have two features(Height and Width), there will be 2 coef\nprint('intercept= ', model.intercept_)\nprint('score= ', model.score(X_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting The Test Data\n* Check below table for weight from test data and predicted weight by our model\n* We will also plot the scatter plot of weight from test data vs predicted weight","metadata":{}},{"cell_type":"code","source":"predictedWeight = pd.DataFrame(model.predict(X_test), columns=['Predicted Weight']) # Create new dataframe of column'Predicted Weight'\nactualWeight = pd.DataFrame(y_test)\nactualWeight = actualWeight.reset_index(drop=True) # Drop the index so that we can concat it, to create new dataframe\ndf_actual_vs_predicted = pd.concat([actualWeight,predictedWeight],axis =1)\ndf_actual_vs_predicted.T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see from above comparison, predicted weights are negative when actual weights are smaller than 20gm\n\nWe can also visualize the above comparison using scatter plots","metadata":{}},{"cell_type":"code","source":"plt.scatter(y_test, model.predict(X_test))\nplt.xlabel('Weight From Test Data')\nplt.ylabel('Weight Predicted By Model')\nplt.rcParams[\"figure.figsize\"] = (10,6) # Custom figure size in inches\nplt.title(\"Weight From test Data Vs Weight Predicted By Model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(X_test['Height'], y_test, color='red', label = 'Actual Weight')\nplt.scatter(X_test['Height'], model.predict(X_test), color='green', label = 'Prdicted Weight')\nplt.xlabel('Height')\nplt.ylabel('Weight')\nplt.rcParams[\"figure.figsize\"] = (10,6) # Custom figure size in inches\nplt.title('Actual Vs Predicted Weight for Test Data')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(X_test['Width'], y_test, color='red', label = 'Actual Weight')\nplt.scatter(X_test['Width'], model.predict(X_test), color='green', label = 'Prdicted Weight')\nplt.xlabel('Width')\nplt.ylabel('Weight')\nplt.rcParams[\"figure.figsize\"] = (10,6) # Custom figure size in inches\nplt.title('Actual Vs Predicted Weight for Test Data')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 6: Evaluating the Model\n\nPlot a histogram of the residuals.","metadata":{}},{"cell_type":"code","source":"sns.distplot((y_test-model.predict(X_test)))\nplt.rcParams[\"figure.figsize\"] = (10,6) # Custom figure size in inches\nplt.title(\"Histogram of Residuals\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n* As you can see from above results our model score is 89.6%, which is good enough to start with.\n* But one issue with prediction is negative weight values. This behavior is true for smaller(less than 20gm) weight values.\n* In machine learning, every time we are solving a problem we make some choices which affect the results.\n* We have also made few choices like treating all species as one since we have small dataset.\n* I will try again with different approach to try an eliminate the negative weight values.","metadata":{}}]}